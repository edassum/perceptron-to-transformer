# perceptron-to-transformer

## From First Principles to Modern Neural Networks

This repository documents my deep learning learning journey â€” starting from the **Perceptron** and gradually progressing toward modern **Transformer-based architectures**.

The focus of this project is **understanding how neural networks work internally**, not just how to use high-level frameworks.  
Every concept is approached from first principles, supported by minimal, readable implementations.

---

## ğŸ¯ Goals

- Build a **strong conceptual foundation**
- Understand the **mathematics behind learning**
- Implement models **from scratch**
- See how modern architectures evolved from simpler ones
- Connect **theory â†” code â†” intuition**

---

## ğŸ§  Learning Philosophy

> **Learn â†’ Implement â†’ Experiment â†’ Understand**

- Learn the idea
- Implement it manually
- Experiment with variations
- Understand *why* it works (or fails)

All implementations aim to be:
- Simple
- Educational
- Explicit
- Easy to modify and extend

---

## ğŸ—‚ï¸ Intended Repository Structure

The code and notes are organized to reflect a **logical learning progression**:

```text
perceptron-to-transformer/
â”‚
â”œâ”€â”€ concepts/        # Theory, math, and intuition
â”‚   â”œâ”€â”€ perceptron.md
â”‚   â”œâ”€â”€ loss-functions.md
â”‚   â”œâ”€â”€ gradient-descent.md
â”‚   â”œâ”€â”€ backpropagation.md
â”‚   â””â”€â”€ activation-functions.md
â”‚
â”œâ”€â”€ fnn/             # Feedforward Neural Networks (MLP)
â”‚   â”œâ”€â”€ single-layer/
â”‚   â”œâ”€â”€ multi-layer/
â”‚   â””â”€â”€ experiments/
â”‚
â”œâ”€â”€ cnn/             # Convolutional Neural Networks
â”‚   â”œâ”€â”€ convolution-from-scratch/
â”‚   â”œâ”€â”€ pooling/
â”‚   â””â”€â”€ cnn-models/
â”‚
â”œâ”€â”€ rnn/             # Recurrent Neural Networks
â”‚   â”œâ”€â”€ vanilla-rnn/
â”‚   â”œâ”€â”€ lstm/
â”‚   â””â”€â”€ gru/
â”‚
â”œâ”€â”€ attention/       # Attention mechanisms
â”‚   â””â”€â”€ self-attention-from-scratch/
â”‚
â”œâ”€â”€ transformers/    # Transformer architecture
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ multi-head-attention/
â”‚   â””â”€â”€ transformer-from-scratch/
â”‚
â””â”€â”€ notes/           # Reflections, comparisons, and insights
